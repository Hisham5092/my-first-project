## 1. **Complete the Requests Library** (High Priority)

You know GET and POST, but there are more HTTP methods and concepts:

### **HTTP Methods:**
```python
import requests

# GET - Retrieve data
response = requests.get('https://api.example.com/users')

# POST - Create new data
response = requests.post('https://api.example.com/users', json={'name': 'John'})

# PUT - Update entire resource
response = requests.put('https://api.example.com/users/1', json={'name': 'John Updated'})

# PATCH - Update part of resource
response = requests.patch('https://api.example.com/users/1', json={'name': 'John'})

# DELETE - Remove resource
response = requests.delete('https://api.example.com/users/1')
```

### **Important Concepts:**

#### **Headers (Authentication, Content-Type)**
```python
# API Key authentication
headers = {
    'Authorization': 'Bearer YOUR_API_KEY',
    'Content-Type': 'application/json'
}
response = requests.get('https://api.example.com/data', headers=headers)
```

#### **Query Parameters**
```python
# Method 1: In URL
response = requests.get('https://api.example.com/users?page=1&limit=10')

# Method 2: Using params (better)
params = {'page': 1, 'limit': 10}
response = requests.get('https://api.example.com/users', params=params)
```

#### **Handling Responses**
```python
response = requests.get('https://api.example.com/data')

# Check status
print(response.status_code)  # 200, 404, 500, etc.

# Get JSON data
data = response.json()

# Get text
text = response.text

# Check if successful
if response.ok:  # status code 200-299
    print("Success!")
```

#### **Error Handling**
```python
import requests

try:
    response = requests.get('https://api.example.com/data', timeout=5)
    response.raise_for_status()  # Raises error for 4xx, 5xx
    data = response.json()
except requests.exceptions.Timeout:
    print("Request timed out")
except requests.exceptions.RequestException as e:
    print(f"Error: {e}")
```

**Time to learn:** 1-2 days

---

## 2. **Working with JSON** (High Priority)

Essential for APIs and automation:

```python
import json

# Parse JSON string to Python dict
json_string = '{"name": "John", "age": 30}'
data = json.loads(json_string)
print(data['name'])  # "John"

# Convert Python dict to JSON string
data = {'name': 'John', 'age': 30}
json_string = json.dumps(data, indent=2)
print(json_string)

# Read JSON from file
with open('data.json', 'r') as f:
    data = json.load(f)

# Write JSON to file
with open('data.json', 'w') as f:
    json.dump(data, f, indent=2)

# Nested JSON navigation
data = {
    'user': {
        'name': 'John',
        'address': {
            'city': 'Dhaka'
        }
    }
}
print(data['user']['address']['city'])  # "Dhaka"

# Handle missing keys safely
city = data.get('user', {}).get('address', {}).get('city', 'Unknown')
```

**Time to learn:** 1 day

---

## 3. **File Handling** (High Priority for Automation)

Reading/writing files is crucial for automation:

### **Text Files:**
```python
# Read file
with open('data.txt', 'r') as f:
    content = f.read()

# Write file
with open('output.txt', 'w') as f:
    f.write("Hello World")

# Append to file
with open('log.txt', 'a') as f:
    f.write("New log entry\n")

# Read line by line
with open('data.txt', 'r') as f:
    for line in f:
        print(line.strip())
```

### **CSV Files:**
```python
import csv

# Read CSV
with open('data.csv', 'r') as f:
    reader = csv.DictReader(f)
    for row in reader:
        print(row['name'], row['email'])

# Write CSV
data = [
    {'name': 'John', 'email': 'john@example.com'},
    {'name': 'Jane', 'email': 'jane@example.com'}
]

with open('output.csv', 'w', newline='') as f:
    writer = csv.DictWriter(f, fieldnames=['name', 'email'])
    writer.writeheader()
    writer.writerows(data)
```

### **Excel Files (openpyxl):**
```python
from openpyxl import load_workbook, Workbook

# Read Excel
wb = load_workbook('data.xlsx')
sheet = wb.active
for row in sheet.iter_rows(min_row=2, values_only=True):
    print(row)

# Write Excel
wb = Workbook()
sheet = wb.active
sheet.append(['Name', 'Email'])
sheet.append(['John', 'john@example.com'])
wb.save('output.xlsx')
```

**Time to learn:** 2-3 days

---

## 4. **Error Handling & Logging** (Medium Priority)

Essential for production automation:

### **Try-Except:**
```python
try:
    result = 10 / 0
except ZeroDivisionError:
    print("Cannot divide by zero")
except Exception as e:
    print(f"Error: {e}")
finally:
    print("This always runs")
```

### **Logging:**
```python
import logging

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    filename='app.log'
)

# Use logging
logging.info("Process started")
logging.warning("This is a warning")
logging.error("An error occurred")

# In automation
try:
    response = requests.get('https://api.example.com/data')
    logging.info(f"API call successful: {response.status_code}")
except Exception as e:
    logging.error(f"API call failed: {e}")
```

**Time to learn:** 1-2 days

---

## 5. **Environment Variables** (High Priority for Security)

Never hardcode API keys!

### **Using python-dotenv:**
```bash
pip install python-dotenv
```

**Create `.env` file:**
```
API_KEY=your_secret_key_here
DATABASE_URL=postgresql://user:pass@localhost/db
```

**Use in code:**
```python
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Access them
api_key = os.getenv('API_KEY')
db_url = os.getenv('DATABASE_URL')

# Use in requests
headers = {'Authorization': f'Bearer {api_key}'}
response = requests.get('https://api.example.com/data', headers=headers)
```

**Time to learn:** 1 day

---

## 6. **Database Basics** (Medium Priority)

For storing automation results:

### **SQLite (Simplest):**
```python
import sqlite3

# Connect to database
conn = sqlite3.connect('automation.db')
cursor = conn.cursor()

# Create table
cursor.execute('''
    CREATE TABLE IF NOT EXISTS users (
        id INTEGER PRIMARY KEY,
        name TEXT,
        email TEXT
    )
''')

# Insert data
cursor.execute('INSERT INTO users (name, email) VALUES (?, ?)',
               ('John', 'john@example.com'))

# Query data
cursor.execute('SELECT * FROM users')
for row in cursor.fetchall():
    print(row)

# Commit and close
conn.commit()
conn.close()
```

### **PostgreSQL/MySQL (Production):**
```python
import psycopg2  # or pymysql for MySQL

conn = psycopg2.connect(
    host="localhost",
    database="mydb",
    user="user",
    password="password"
)
```

**Time to learn:** 3-5 days

---

## 7. **Web Scraping** (Good for Automation)

Extract data from websites:

### **BeautifulSoup:**
```python
import requests
from bs4 import BeautifulSoup

# Fetch webpage
response = requests.get('https://example.com')
soup = BeautifulSoup(response.text, 'html.parser')

# Find elements
title = soup.find('h1').text
links = soup.find_all('a')

for link in links:
    print(link.get('href'))

# Find by class
items = soup.find_all('div', class_='product')
```

### **Selenium (For JavaScript-heavy sites):**
```python
from selenium import webdriver
from selenium.webdriver.common.by import By

driver = webdriver.Chrome()
driver.get('https://example.com')

# Find element
element = driver.find_element(By.ID, 'username')
element.send_keys('myusername')

# Click button
button = driver.find_element(By.CSS_SELECTOR, '.submit-btn')
button.click()

driver.quit()
```

**Time to learn:** 3-5 days

---

## 8. **Schedule Tasks** (For Automation)

Run your scripts automatically:

### **schedule library:**
```python
import schedule
import time

def job():
    print("Running automation task...")
    # Your automation code here

# Run every day at 10:00
schedule.every().day.at("10:00").do(job)

# Run every hour
schedule.every().hour.do(job)

# Run every 10 minutes
schedule.every(10).minutes.do(job)

while True:
    schedule.run_pending()
    time.sleep(1)
```

### **Cron (Linux/Mac):**
```bash
# Edit crontab
crontab -e

# Run script daily at 9 AM
0 9 * * * /usr/bin/python3 /path/to/script.py
```

**Time to learn:** 1-2 days

---

## 9. **APIs You Should Practice With**

Build projects with these free APIs:

1. **Google Sheets API** - Read/write spreadsheets
2. **Airtable API** - Database automation
3. **Slack API** - Send notifications
4. **Telegram Bot API** - Create bots
5. **SendGrid/Mailgun** - Send emails
6. **Twilio** - Send SMS
7. **Stripe API** - Payment processing
8. **GitHub API** - Repository automation
9. **OpenWeatherMap** - Weather data
10. **News API** - News aggregation

---

## 10. **Version Control (Git)** (Essential)

You should already know this for any job:

```bash
# Initialize repo
git init

# Add files
git add .

# Commit
git commit -m "Initial commit"

# Push to GitHub
git remote add origin https://github.com/user/repo.git
git push -u origin main

# Create branch
git checkout -b feature-branch

# Merge
git checkout main
git merge feature-branch
```

**Time to learn:** 2-3 days

---

## Learning Path Recommendation:

### **Week 1-2: APIs & Data Handling**
- Complete requests library (all methods)
- JSON parsing and manipulation
- File handling (CSV, Excel, text)
- Environment variables

### **Week 3-4: Error Handling & Storage**
- Error handling best practices
- Logging
- SQLite basics
- Git/GitHub

### **Week 5-6: Advanced Automation**
- Web scraping (BeautifulSoup)
- Task scheduling
- Build 2-3 automation projects

### **Week 7-8: Build Portfolio Projects**
- Invoice parser with email
- Social media scheduler
- Data pipeline project
- Web scraper with database

---

## Suggested Projects (Apply What You Learn):

### **Project 1: Email Invoice Parser**
- Read emails from Gmail API
- Extract invoice data with regex
- Save to Google Sheets
- Send Slack notification

### **Project 2: Weather Alert Bot**
- Fetch weather from OpenWeatherMap API
- Check conditions (rain, temperature)
- Send SMS via Twilio if alert
- Run every hour with schedule

### **Project 3: Social Media Dashboard**
- Scrape data from multiple platforms
- Store in SQLite database
- Generate daily report
- Send email with summary

### **Project 4: E-commerce Price Tracker**
- Scrape product prices daily
- Store in database
- Alert when price drops
- Visualize trends

---

## My Recommended Order (For You):

1. **Complete requests library** (1 week)
2. **JSON handling** (2-3 days)
3. **File handling (CSV, Excel)** (1 week)
4. **Error handling & logging** (3 days)
5. **Environment variables** (1 day)
6. **Build a project** combining all above
7. **Web scraping** (1 week)
8. **Database basics** (SQLite) (1 week)
9. **Task scheduling** (2-3 days)
10. **Build 2-3 portfolio projects**

**Total time: 8-10 weeks** to be job-ready for automation roles

---

## Resources:

- **Requests docs:** https://requests.readthedocs.io/
- **Real Python tutorials:** https://realpython.com/
- **Automate the Boring Stuff:** https://automatetheboringstuff.com/
- **YouTube:** Corey Schafer's Python tutorials

---

Which topic would you like to start with next? I can give you detailed tutorials and practice exercises! 🚀